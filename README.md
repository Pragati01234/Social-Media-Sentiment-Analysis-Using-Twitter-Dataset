# Social-Media-Sentiment-Analysis-Using-Twitter-Dataset
Social media platforms like Twitter, Facebook, YouTube, Reddit generate huge amounts of big data that can be mined in various ways to understand trends, public sentiments, and opinions. Social media data today has become relevant for branding, marketing, and business as a whole. A sentiment analyzer learns about various sentiments behind a “content piece”  (could be IM, email, tweet, or any other social media post) through machine learning and predicts the same using AI.Twitter data is considered a definitive entry point for beginners to practice sentiment analysis machine learning problems. 

When I first researched about sentiment analysis, it seemed that most of the resources/artices on the subject were about academic and clean datasets. For instance there are hundreds of tutorials out there about how to classify movie reviews from the super-popular IMDB dataset. Yet information about real world sentiment analysis, like how to deal with messy social media messages, is hard to find.

I assume this is because people tend to gravitate towards commonly-used datasets, but also because dealing with social media messages is a tough problem that has not been fully cracked yet. Sarcasm, humour, subjectivity, smileys... there are many challenges along the way.

The good news is that there has been major improvements of the standard techniques in the NLP toolbox during the past few years, notably with the introduction of large pre-trained models based on the Transformer architecture like BERT. These new models enable much richer and comprehensive representations of messages which can be used to classify sentiment more accurately.

I wrote this post to share some of what I learned in my journey building BrandImage.io, an online tool to better understand what people like/dislike about a given company and its products. I hope you'll find it useful as a starting point to learn how to build robust pipelines to deal with social media messages.

The goal here is not to beat the state-of-the-art accuracy but to offer a practical guide to the problem, so we will focus on straightforward and scalable solutions. Most attention will be given to different ways of pre-processing and representing the messages for classification, which in my experience is where 80% of the performance improvement happens! We will not build a large/deep model for the classification as this would make deployment harder. However we will look at BERT embeddings towards the end if you are interested in using more advanced representations.
